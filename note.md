## 令人拍案叫绝的Wasserstein GAN

https://zhuanlan.zhihu.com/p/25071913

刚才谈到很多 GAN 的优点、应用和变种，但是GAN并不是完美无缺的。

其实使用过 GAN 的人应该知道，训练 GAN 有很多头疼的问题。例如：GAN 的训练对超参数特别敏感，需要精心设计。GAN 中关于生成模型和判别模型的迭代也很有问题，按照通常理解，如果判别模型训练地很好，应该对生成的提高有很大作用，但实际中恰恰相反，如果将判别模型训练地很充分，生成模型甚至会变差。那么问题出在哪里呢？

其实问题在于目标函数的设计上。

在 ICLR 2017 大会上有一篇口头报告论文提出了这个问题产生的机理和解决办法。这篇文章的作者(Martin Arjovsky)证明，GAN的本质其实是优化真实样本分布和生成样本分布之间的差异，并最小化这个差异。特别需要指出的是，优化的目标函数是两个分布上的 Jensen-Shannon 距离，但这个距离有这样一个问题，如果两个分布的样本空间并不完全重合，这个距离是无法定义的。

作者接着证明了 “真实分布与生成分布的样本空间并不完全重合” 是一个极大概率事件，并证明在一些假设条件下，可以从理论层面推导出一些实际中遇到的现象。

1. Wasserstein GAN（下面简称 WGAN）成功地做到了以下爆炸性的几点：

* 彻底解决 GAN 训练不稳定的问题，不再需要小心平衡生成器和判别器的训练程度

* 基本解决了 collapse mode 的问题，确保了生成样本的多样性 
  
* 训练过程中终于有一个像交叉熵、准确率这样的数值来指示训练的进程，这个数值越小代表 GAN 训练得越好，代表生成器产生的图像质量越高（如图所示）
![image](https://pic4.zhimg.com/v2-3cfe84e6b6b58c00e013975fe649398e_1200x500.jpg)

* 以上一切好处不需要精心设计的网络架构，最简单的多层全连接网络就可以做到

2. 算法改进后相比原始GAN的算法实现流程却只改了四点：

* 判别器最后一层去掉sigmoid
* 生成器和判别器的loss不取log
* 每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c
* 不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行

3. 原始GAN的问题所在
   
   
